[CLS] [SEP] Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. [CLS] [SEP] Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). [CLS] [SEP] Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. [CLS] [SEP] For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. [CLS] [SEP] Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics. [CLS] [SEP] For example, the word shot in “It was a nice shot.” should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. [CLS] [SEP] Nida (1964) stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. [CLS] [SEP] In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs. [CLS] [SEP] With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. [CLS] [SEP] Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. [CLS] [SEP] This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. [CLS] [SEP] These approaches can be expensive, and they do not emphasize stochastic translation aspects. [CLS] [SEP] Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. [CLS] [SEP] We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. [CLS] [SEP] Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). [CLS] [SEP] Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. [CLS] [SEP] In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object. [CLS] [SEP] Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent. [CLS] [SEP] Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models 969 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006. [CLS] [SEP] Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts. [CLS] [SEP] In this paper we investigate three instances of the BiTAM model, They are data-driven and do not need handcrafted knowledge engineering. [CLS] [SEP] The remainder of the paper is as follows: in section 2, we introduce notations and baselines; in section 3, we propose the topic admixture models; in section 4, we present the learning and inference algorithms; and in section 5 we show experiments of our models. [CLS] [SEP] We conclude with a brief discussion in section 6. [CLS] [SEP] In statistical machine translation, one typically uses parallel data to identify entities such as “word-pair”, “sentence-pair”, and “document- pair”. [CLS] [SEP] Formally, we define the following terms1: • A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. • A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.• A document-pair (F, E) refers to two doc uments which are translations of each other. [CLS] [SEP] Assuming sentences are one-to-one correspondent, a document-pair has a sequence of N parallel sentence-pairs {(fn, en)}, where (fn, en) is the ntth parallel sentence-pair. [CLS] [SEP] • A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}. [CLS] [SEP] 2.1 Baseline: IBM Model-1. [CLS] [SEP] The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level. [CLS] [SEP] The translation lexicon p(f |e) is the key component in this generative process. [CLS] [SEP] An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004). [CLS] [SEP] We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework. [CLS] [SEP] Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity. [CLS] [SEP] In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair. [CLS] [SEP] A unique normalized and real-valued vector θ, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions. [CLS] [SEP] Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments. [CLS] [SEP] Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. [CLS] [SEP] Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled. [CLS] [SEP] There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels. [CLS] [SEP] We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels. [CLS] [SEP] J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e). [CLS] [SEP] (1) j=1 i=1 1 We follow the notations in (Brown et al., 1993) for. [CLS] [SEP] English-French, i.e., e ↔ f , although our models are tested,in this paper, for EnglishChinese. [CLS] [SEP] We use the end-user ter minology for source and target languages. [CLS] [SEP] In the first BiTAM model, we assume that topics are sampled at the sentence-level. [CLS] [SEP] Each document- pair is represented as a random mixture of latent topics. [CLS] [SEP] Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I β e I a α θ z f J B N M α θ z a a f J B α θ z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs. [CLS] [SEP] A node in the graph represents a random variable, and a hexagon denotes a parameter. [CLS] [SEP] Un-shaded nodes are hidden variables. [CLS] [SEP] All the plates represent replicates. [CLS] [SEP] The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair. [CLS] [SEP] (a) BiTAM1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair. [CLS] [SEP] a translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), where z is an indicator variable to denote the choice of a topic. [CLS] [SEP] Given a specific topic-weight vector θd for a document-pair, each sentence-pair draws its conditionally independent topics from a mixture of topics. [CLS] [SEP] This generative process, for a document-pair (Fd, Ed), is summarized as below: 1. [CLS] [SEP] Sample sentence-number N from a Poisson(γ).. [CLS] [SEP] 2. [CLS] [SEP] Sample topic-weight vector θd from a Dirichlet(α).. [CLS] [SEP] 3. [CLS] [SEP] For each sentence-pair (fn , en ) in the dtth doc-pair ,. [CLS] [SEP] (a) Sample sentence-length Jn from Poisson(δ); (b) Sample a topic zdn from a Multinomial(θd ); (c) Sample ej from a monolingual model p(ej );(d) Sample each word alignment link aj from a uni form model p(aj ) (or an HMM); (e) Sample each fj according to a topic-specific graphical model representation for the BiTAM generative scheme discussed so far. [CLS] [SEP] Note that, the sentence-pairs are now connected by the node θd. Therefore, marginally, the sentence-pairs are not independent of each other as in traditional SMT models, instead they are conditionally independent given the topic-weight vector θd. Specifically, BiTAM1 assumes that each sentence-pair has one single topic. [CLS] [SEP] Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair. [CLS] [SEP] The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B). [CLS] [SEP] and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear. [CLS] [SEP] For each document-pair, a K -dimensional Dirichlet random variable θd, referred to as the topic-weight vector of the document, can take values in the (K −1)-simplex following a probability density: to the proposed distributions. [CLS] [SEP] We simplify alignment model of a, as in IBM1, by assuming that aj is sampled uniformly at random. [CLS] [SEP] Given the parameters α, B, and the English part E, the joint conditional distribution of the topic-weight vector θ, the topic indicators z, the alignment vectors A, and the document F can be written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) where the hyperparameter α is a K -dimension vector with each component αk >0, and Γ(x) is the Gamma function. [CLS] [SEP] The alignment is represented by a J -dimension vector a = {a1, a2, · · · , aJ }; for each French word fj at the position j, an position variable aj maps it to anEnglish word eaj at the position aj in English sen p(θ | α) n p(zn |θ)p(fn , an |en , α, Bzn), n=1 where N is the number of the sentence-pair. [CLS] [SEP] Marginalizing out θ and z, we can obtain the marginal conditional probability of generating F from E for each document-pair: p(F, A|E, α, Bzn ) = tence. [CLS] [SEP] The word level translation lexicon probabil- r ( (5) ities are topic-specific, and they are parameterized by the matrix B = {Bk }. [CLS] [SEP] p(θ|α) n) p(zn |θ)p(fn , an |en , Bzn ) dθ, n=1 zn For simplicity, in our current models we omit the modelings of the sentence-number N and the sentence-length Jn, and focus only on the bilingual translation model. [CLS] [SEP] Figure 1 (a) shows the where p(fn, an|en, Bzn ) is a topic-specific sentence-level translation model. [CLS] [SEP] For simplicity, we assume that the French words fj ’s are conditionally independent of each other; the alignment variables aj ’s are independent of other variables and are uniformly distributed a priori. [CLS] [SEP] Therefore, the distribution for each sentence-pair is: p(fn , an |en , Bzn) = p(fn |en , an , Bzn)p(an |en , Bzn) Jn “Null” is attached to every target sentence to align the source words which miss their translations. [CLS] [SEP] Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ). [CLS] [SEP] (6) contains only one word: “Null”, and the alignment link a is no longer a hidden variable. [CLS] [SEP] Thus, the conditional likelihood for the entire parallel corpus is given by taking the product of the marginal probabilities of each individual document-pair in Eqn. [CLS] [SEP] 5. [CLS] [SEP] 3.2 BiTAM2: Monolingual Admixture. [CLS] [SEP] In general, the monolingual model for English can also be a rich topic-mixture. [CLS] [SEP] This is realized by using the same topic-weight vector θd and the same topic indicator zdn sampled according to θd, as described in §3.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)). [CLS] [SEP] Now e is generated [CLS] [SEP] Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and θ is intractable. [CLS] [SEP] A variational inference is used to approximate the true posteriors of these hidden variables. [CLS] [SEP] The inference scheme is presented for BiTAM1; the algorithms for BiTAM2 and BiTAM3 are straight forward extensions and are omitted. [CLS] [SEP] 4.1 Variational Approximation. [CLS] [SEP] To approximate: p(θ, z, A|E, F, α, B), the joint posterior, we use the fully factorized distribution over the same set of hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· from a topic-based language model β, instead of a N Jn (7) uniform distribution in BiTAM1. [CLS] [SEP] We refer to this n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model as BiTAM2. [CLS] [SEP] n=1 j=1 Unlike BiTAM1, where the information observed in ei is indirectly passed to z via the node of fj and the hidden variable aj , in BiTAM2, the topics of corresponding English and French sentences are also strictly aligned so that the information observed in ei can be directly passed to z, in the hope of finding more accurate topics. [CLS] [SEP] The topics are inferred more directly from the observed bilingual data, and as a result, improve alignment. [CLS] [SEP] 3.3 BiTAM3: Word-level Admixture. [CLS] [SEP] where the Dirichlet parameter γ, the multinomial parameters (φ1, · · · , φn), and the parameters (ϕn1, · · · , ϕnJn ) are known as variational param eters, and can be optimized with respect to the KullbackLeibler divergence from q(·) to the original p(·) via an iterative fixed-point algorithm. [CLS] [SEP] It can be shown that the fixed-point equations for the variational parameters in BiTAM1 are as follows: Nd γk = αk + ) φdnk (8) n=1 K It is straightforward to extend the sentence-level BiTAM1 to a word-level admixture model, by φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j for each word-pair (fj , eaj ) in the ntth sentence-pair, rather than once for all (words) in the sentence (Figure 1 (c)). [CLS] [SEP] exp ( ) ) ϕdnji log Bf ,e ,k (9) j i j=1 i=1 K ( This gives rise to our BiTAM3. [CLS] [SEP] The conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions can be obtained by extending where Ψ(·) is a digamma function. [CLS] [SEP] Note that inthe formulas in §3.1 to move the variable zn,j in side the same loop over each of the fn,j . the above formulas φ dnkis the variational param 3.4 Incorporation of Word “Null”. [CLS] [SEP] Similar to IBM models, “Null” word is used for the source words which have no translation counterparts in the target language. [CLS] [SEP] For example, Chinese words “de” (ffl) , “ba” (I\) and “bei” (%i) generally do not have translations in English. [CLS] [SEP] eter underlying the topic indicator zdn of the nth sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair. [CLS] [SEP] Following a variational EM scheme (Beal and Ghahramani, 2002), we estimate the model parameters α and B in an unsupervised fashion. [CLS] [SEP] Essentially, Eqs. [CLS] [SEP] (810) above constitute the E-step, where the posterior estimations of the latent variables are obtained. [CLS] [SEP] In the M-step, we update α and B so that they improve a lower bound of the log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)]. [CLS] [SEP] (11) The close-form iterative updating formula B is: BDA selects iteratively, for each f , the best aligned e, such that the word-pair (f, e) is the maximum of both row and column, or its neighbors have more aligned pairs than the other combpeting candidates.A close check of {ϕdnji} in Eqn. [CLS] [SEP] 10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) d n=1 j=1 i=1 For α, close-form update is not available, and we resort to gradient accent as in (Sjo¨ lander et al., 1996) with restarts to ensure each updated αk >0. [CLS] [SEP] 4.2 Data Sparseness and Smoothing. [CLS] [SEP] The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). [CLS] [SEP] To reduce the data sparsity problem, we introduce two remedies in our models. [CLS] [SEP] First: Laplace smoothing. [CLS] [SEP] In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory. [CLS] [SEP] Second: interpolation smoothing. [CLS] [SEP] Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e). [CLS] [SEP] (13) As in Eqn. [CLS] [SEP] 1, p(f |e) is learned via IBM1; λ is estimated via EM on held out data. [CLS] [SEP] 4.3 Retrieving Word Alignments. [CLS] [SEP] Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA). [CLS] [SEP] Both use the posterior mean of the alignment indicators adnji, captured by what we call the poste rior alignment matrix ϕ ≡ {ϕdnji}. [CLS] [SEP] UDA uses a French word fdnj (at the jtth position of ntth sentence in the dtth document) to query ϕ to get the best aligned English word (by taking the maximum point in a row of ϕ): adnj = arg max ϕdnji . [CLS] [SEP] (14) i∈[1,Idn ] icon’s strength. [CLS] [SEP] We evaluate BiTAM models on the word alignment accuracy and the translation quality. [CLS] [SEP] For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality, Bleu (Papineni et al., 2002) and its variation of NIST scores are reported. [CLS] [SEP] Table 1: Training and Test Data Statistics Tra in #D oc. [CLS] [SEP] #S ent . #T ok en s En gli sh Ch ine se Tr ee b a n k F B IS . B J Si n or a m a Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes t 95 62 7 25, 50 0 19, 72 6 We have two training data settings with different sizes (see Table 1). [CLS] [SEP] The small one consists of 316 document-pairs from Tree- bank (LDC2002E17). [CLS] [SEP] For the large training data setting, we collected additional document- pairs from FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), and Xinhua News (LDC2002E18, document boundaries are kept in our sentence-aligner (Zhao and Vogel, 2002)). [CLS] [SEP] There are 27,940 document-pairs, containing 327K sentence-pairs or 12 million (12M) English tokens and 11M Chinese tokens. [CLS] [SEP] To evaluate word alignment, we hand-labeled 627 sentence-pairs from 95 document-pairs sampled from TIDES’01 dryrun data. [CLS] [SEP] It contains 14,769 alignment-links. [CLS] [SEP] To evaluate translation quality, TIDES’02 Eval. [CLS] [SEP] test is used as development set, and TIDES’03 Eval. [CLS] [SEP] test is used as the unseen test data. [CLS] [SEP] 5.1 Model Settings. [CLS] [SEP] First, we explore the effects of Null word and smoothing strategies. [CLS] [SEP] Empirically, we find that adding “Null” word is always beneficial to all models regardless of number of topics selected. [CLS] [SEP] To pics Le xic ons To pic1 To pic2 To pic3 Co oc. [CLS] [SEP] IBM 1 H M M IBM 4 p( Ch ao Xi an (Ji! [CLS] [SEP] $) |K ore an) 0. [CLS] [SEP] 06 12 0. [CLS] [SEP] 21 38 0. [CLS] [SEP] 22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li! [CLS] [SEP] � )|K ore an) 0. [CLS] [SEP] 83 79 0. [CLS] [SEP] 61 16 0. [CLS] [SEP] 02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1. [CLS] [SEP] The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (Ji!$:North Korean). [CLS] [SEP] The co-occurrence (Cooc), IBM1&4 and HMM only prefer to translate into HanGuo (li!�:South Korean). [CLS] [SEP] The two candidate translations may both fade out in the learned translation lexicons. [CLS] [SEP] Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h i n a u . s . dev elop men t trad e ente rpri ses tech nolo gy cou ntri es y e a r eco nom ic Topi c B. cho ngqi ng com pani es take over s co m pa ny cit y bi lli o n m o r e eco nom ic re a c h e d y u a n Topi c C. sp or ts dis abl ed te a m p e o p l e caus e w at e r na tio na l ga m es han dica ppe d me mb ers Table 3: Three most distinctive topics are displayed. [CLS] [SEP] The English words for each topic are ranked according to p(e|z) estimated from the topic-specific English sentences weighted by {φdnk }. [CLS] [SEP] 33 functional words were removed to highlight the main content of each topic. [CLS] [SEP] Topic A is about Us-China economic relationships; Topic B relates to Chinese companies’ merging; Topic C shows the sports of handicapped people.The interpolation smoothing in §4.2 is effec tive, and it gives slightly better performance than Laplace smoothing over different number of topics for BiTAM1. [CLS] [SEP] However, the interpolation leverages the competing baseline lexicon, and this can blur the evaluations of BiTAM’s contributions. [CLS] [SEP] Laplace smoothing is chosen to emphasize more on BiTAM’s strength. [CLS] [SEP] Without any smoothing, F- measure drops very quickly over two topics. [CLS] [SEP] In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models. [CLS] [SEP] We train, for comparison, IBM1&4 and HMM models with 8 iterations of IBM1, 7 for HMM and 3 for IBM4 (18h743) with Null word and a maximum fertility of 3 for ChineseEnglish. [CLS] [SEP] Choosing the number of topics is a model selection problem. [CLS] [SEP] We performed a tenfold cross- validation, and a setting of three-topic is chosen for both the small and the large training data sets. [CLS] [SEP] The overall computation complexity of the BiTAM is linear to the number of hidden topics. [CLS] [SEP] 5.2 Variational Inference. [CLS] [SEP] Under a non-symmetric Dirichlet prior, hyperparameter α is initialized randomly; B (K translation lexicons) are initialized uniformly as did in IBM1. [CLS] [SEP] Better initialization of B can help to avoid local optimal as shown in § 5.5. [CLS] [SEP] With the learned B and α fixed, the variational parameters to be computed in Eqn. [CLS] [SEP] (810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10−5. [CLS] [SEP] The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs. [CLS] [SEP] To estimate B, β (for BiTAM2) and α, at most eight variational EM iterations are run on the training data. [CLS] [SEP] Figure 2 shows absolute 2∼3% better F-measure over iterations of variational EM using two and three topics of BiTAM1 comparing with IBM1. [CLS] [SEP] BiTam with Null and Laplace Smoothing Over Var. [CLS] [SEP] EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2. [CLS] [SEP] IB M −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number of EM/Variational EM Iterations for IBM−1 and BiTam−1 Figure 2: performances over eight Variational EM iterations of BiTAM1 using both the “Null” word and the laplace smoothing; IBM1 is shown over eight EM iterations for comparison. [CLS] [SEP] 5.3 Topic-Specific Translation. [CLS] [SEP] Lexicons The topic-specific lexicons Bk are smaller in size than IBM1, and, typically, they contain topic trends. [CLS] [SEP] For example, in our training data, North Korean is usually related to politics and translated into “ChaoXian” (Ji! [CLS] [SEP] $); South Korean occurs more often with economics and is translated as “HanGuo”(li! [CLS] [SEP] �). [CLS] [SEP] BiTAMs discriminate the two by considering the topics of the context. [CLS] [SEP] Table 2 shows the lexicon entries for “Korean” learned by a 3-topic BiTAM1. [CLS] [SEP] The values are relatively sharper, and each clearly favors one of the candidates. [CLS] [SEP] The co-occurrence count, however, only favors “HanGuo”, and this can easily dominate the decisions of IBM and HMM models due to their ignorance of the topical context. [CLS] [SEP] Monolingual topics learned by BiTAMs are, roughly speaking, fuzzy especially when the number of topics is small. [CLS] [SEP] With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3. [CLS] [SEP] 5.4 Evaluating Word. [CLS] [SEP] Alignments We evaluate word alignment accuracies in various settings. [CLS] [SEP] Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE). [CLS] [SEP] Additional heuristics are applied to further improve the accuracies. [CLS] [SEP] Inter takes the intersection of the two directions and generates high-precision alignments; the SE T TI N G IBM 1 H M M IBM 4 B I T A M 1 U D A BDA B I T A M 2 U D A BDA B I T A M 3 U D A BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E D ( % ) U N I O N ( % ) IN TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N I S T B L E U 6. [CLS] [SEP] 45 8 15 .7 0 6. [CLS] [SEP] 82 2 17 .7 0 6. [CLS] [SEP] 92 6 18 .2 5 6. [CLS] [SEP] 93 7 6.954 17 .93 18.14 6. [CLS] [SEP] 90 4 6.976 18 .13 18.05 6. [CLS] [SEP] 96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1. [CLS] [SEP] For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality. [CLS] [SEP] Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments. [CLS] [SEP] As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1∼3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1. [CLS] [SEP] A close look at the three BiTAMs does not yield significant difference. [CLS] [SEP] BiTAM3 is slightly better in most settings; BiTAM1 is slightly worse than the other two, because the topics sampled at the sentence level are not very concentrated. [CLS] [SEP] The BDA align ments of BiTAM1∼3 yield 48.26%, 48.63% and 49.02%, which are even better than HMM and IBM4 — their best performances are at 44.26% and 45.96%, respectively. [CLS] [SEP] This is because BDA partially utilizes similar heuristics on the approximated posterior matrix {ϕdnji} instead of di rect operations on alignments of two directions in the heuristics of Refined. [CLS] [SEP] Practically, we also apply BDA together with heuristics for IBM1, HMM and IBM4, and the best achieved performances are at 40.56%, 46.52% and 49.18%, respectively. [CLS] [SEP] Overall, BiTAM models achieve performances close to or higher than HMM, using only a very simple IBM1 style alignment model. [CLS] [SEP] Similar improvements over IBM models and HMM are preserved after applying the three kinds of heuristics in the above. [CLS] [SEP] As expected, since BDA already encodes some heuristics, it is only slightly improved with the Union heuristic; UDA, similar to the viterbi style alignment in IBM and HMM, is improved better by the Refined heuristic. [CLS] [SEP] We also test BiTAM3 on large training data, and similar improvements are observed over those of the baseline models (see Table. [CLS] [SEP] 5). [CLS] [SEP] 5.5 Boosting BiTAM Models. [CLS] [SEP] The translation lexicons of Bf,e,k are initialized uniformly in our previous experiments. [CLS] [SEP] Better ini tializations can potentially lead to better performances because it can help to avoid the undesirable local optima in variational EM iterations. [CLS] [SEP] We use the lexicons from IBM Model-4 to initialize Bf,e,k to boost the BiTAM models. [CLS] [SEP] This is one way of applying the proposed BiTAM models into current state-of-the-art SMT systems for further improvement. [CLS] [SEP] The boosted alignments are denoted as BUDA and BBDA in Table. [CLS] [SEP] 5, corresponding to the uni-direction and bi-direction alignments, respectively. [CLS] [SEP] We see an improvement in alignment quality. [CLS] [SEP] 5.6 Evaluating Translations. [CLS] [SEP] To further evaluate our BiTAM models, word alignments are used in a phrase-based decoder for evaluating translation qualities. [CLS] [SEP] Similar to the Pharoah package (Koehn, 2004), we extract phrase-pairs directly from word alignment together with coherence constraints (Fox, 2002) to remove noisy ones. [CLS] [SEP] We use TIDES Eval’02 CE test set as development data to tune the decoder parameters; the Eval’03 data (919 sentences) is the unseen data. [CLS] [SEP] A trigram language model is built using 180 million English words. [CLS] [SEP] Across all the reported comparative settings, the key difference is the bilingual ngram-identity of the phrase-pair, which is collected directly from the underlying word alignment. [CLS] [SEP] Shown in Table 4 are results for the small- data track; the large-data track results are in Table 5. [CLS] [SEP] For the small-data track, the baseline Bleu scores for IBM1, HMM and IBM4 are 15.70, 17.70 and 18.25, respectively. [CLS] [SEP] The UDA alignment of BiTAM1 gives an improvement over the baseline IBM1 from 15.70 to 17.93, and it is close to HMM’s performance, even though BiTAM doesn’t exploit any sequential structures of words. [CLS] [SEP] The proposed BiTAM2 and BiTAM 3 are slightly better than BiTAM1. [CLS] [SEP] Similar improvements are observed for the large-data track (see Table 5). [CLS] [SEP] Note that, the boosted BiTAM3 us SE T TI N G IBM 1 H M M IBM 4 B I T A M 3 U D A BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E D ( % ) U N I O N ( % ) I N T E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N I S T B L E U 7. [CLS] [SEP] 5 9 19 .1 9 7. [CLS] [SEP] 7 7 21 .9 9 7. [CLS] [SEP] 8 3 23 .1 8 7. [CLS] [SEP] 64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models, IBM Models, HMMs, and boosted BiTAMs using all the training data listed in Table. [CLS] [SEP] 1. [CLS] [SEP] Other experimental conditions are similar to Table. [CLS] [SEP] 4. [CLS] [SEP] ing IBM4 as the seed lexicon, outperform the Refined IBM4: from 23.18 to 24.07 on Bleu score, and from 7.83 to 8.23 on NIST. [CLS] [SEP] This result suggests a straightforward way to leverage BiTAMs to improve statistical machine translations. [CLS] [SEP] In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models. [CLS] [SEP] Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models. [CLS] [SEP] The proposed models significantly improve the alignment accuracy and lead to better translation qualities. [CLS] [SEP] Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.