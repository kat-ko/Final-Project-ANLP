[CLS] [SEP] Estimation of Probabilistic Context-Free Grammars [CLS] [SEP] The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one. [CLS] [SEP] We show here that estimated production probabilities always yield proper distributions. [CLS] [SEP] If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees? [CLS] [SEP] We will show that in both cases the estimated probability is tight. [CLS] [SEP] (8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H &lt;B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, &quot;relative frequency,&quot; estimator. [CLS] [SEP] Letting ~y denote {w Efk Y(w) = Y}, the likelihood of the corpus becomes n H E H P(A--&apos;~oL)f(A~;~)&quot; i=1 ~OE~y(~i) (A---~o~)ER And the maximum-likelihood equation becomes + p(B fl) Ei=l EwEfly(wi,I-I(A--.)cR p(A -~ a)f(A-~&quot;;~) = 0 fT(B ~ /3) = ~iL1 Ep~f(B ~ fl;w)lw E ~y(~,)] (4) ,~s,,, ~Ei=IEpV(&quot; a;w)lw E ~Y(o~,)] E(B_~,E B ~ where E~ is expectation under fi and where &quot;]w E~-~y(wi)&quot; means &quot;conditioned on 0.2E ~-~Y(wi)&apos;&quot; There is no hope for a closed form solution, but (4) does suggest an iteration scheme, which, as it turns out, &quot;climbs&quot; the likelihood surface (though there are no guarantees about approaching a global maximum): Let P0 be an arbitrary assignment respecting (1). [CLS] [SEP] Dempster, Laird, and Rubin [1977] put the idea into a much more general setting and coined the Chi and Geman Probabilistic Context-Free Grammars term EM for Expectation-Maximization.