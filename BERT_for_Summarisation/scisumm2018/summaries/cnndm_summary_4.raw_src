[CLS] [SEP] Better Arabic Parsing: Baselines, Evaluations, and Analysis [CLS] [SEP] Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â€“5% F1. [CLS] [SEP] To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results. [CLS] [SEP] The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). [CLS] [SEP] Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (§3). [CLS] [SEP] We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (§4). [CLS] [SEP] Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (Â§6). [CLS] [SEP] When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct � ?f iDafa. [CLS] [SEP] We propose a limit of 70 words for Arabic parsing evaluations. [CLS] [SEP] We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c). [CLS] [SEP] In our grammar, features are realized as annotations to basic category labels. [CLS] [SEP] We start with noun features since written Arabic contains a very high proportion of NPs. [CLS] [SEP] 8 We use head-finding rules specified by a native speaker. [CLS] [SEP] mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences. [CLS] [SEP] Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. [CLS] [SEP] At the phrasal level, we remove all function tags and traces. [CLS] [SEP] The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-. [CLS] [SEP] able at http://nlp.stanford.edu/projects/arabic.shtml. [CLS] [SEP] 6 Joint Segmentation and Parsing. [CLS] [SEP] But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. [CLS] [SEP] Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. [CLS] [SEP] We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. [CLS] [SEP] Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths ≤ 70 (dev set, gold segmentation). [CLS] [SEP] Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. [CLS] [SEP] Table 9: Dev set results for sentences of length ≤ 70. [CLS] [SEP] By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.