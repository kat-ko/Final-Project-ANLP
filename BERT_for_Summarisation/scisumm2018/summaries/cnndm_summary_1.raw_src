[CLS] [SEP] It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. [CLS] [SEP] Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. [CLS] [SEP] In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data. [CLS] [SEP] A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. [CLS] [SEP] We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. [CLS] [SEP] We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information). [CLS] [SEP] Such constraints are derived from training data, expressing some relationship between features and outcome. [CLS] [SEP] The features we used can be divided into 2 classes: local and global. [CLS] [SEP] Local features are features that are based on neighboring tokens, as well as the token itself. [CLS] [SEP] Local features are features that are based on neighboring tokens, as well as the token itself. [CLS] [SEP] Global features are extracted from other occurrences of the same token in the whole document.  [CLS] [SEP] In the maximum entropy framework, there is no such constraint. [CLS] [SEP] Multiple features can be used for the same token. [CLS] [SEP] Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. [CLS] [SEP] For all lists except locations, the lists are processed into a list of tokens (unigrams). [CLS] [SEP] Location list is processed into a list of unigrams and bigrams (e.g., New York). [CLS] [SEP] For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams. [CLS] [SEP] The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps. [CLS] [SEP] For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own. [CLS] [SEP] We have shown that the maximum entropy framework is able to use global information directly.