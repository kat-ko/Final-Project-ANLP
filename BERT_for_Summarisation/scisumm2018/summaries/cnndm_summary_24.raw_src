[CLS] [SEP] Robust pronoun resolution with limited knowledge [CLS] [SEP] Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. [CLS] [SEP] One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task. [CLS] [SEP] This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. [CLS] [SEP] Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. [CLS] [SEP] Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. [CLS] [SEP] Evaluation reports a success rate of 89.7% which is better than the suc­ cess rates of the approaches selected for comparison and tested on the same data. [CLS] [SEP] For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979). [CLS] [SEP] Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph­ ora resolution. [CLS] [SEP] It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. [CLS] [SEP] With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. [CLS] [SEP] It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;). [CLS] [SEP] The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the reÂ­ maining candidates (see next section). [CLS] [SEP] The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im­ mediate reference. [CLS] [SEP] If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. [CLS] [SEP] Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. [CLS] [SEP] Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante­ cedent. [CLS] [SEP] The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. [CLS] [SEP] The collocation preference here is restricted to the patterns &quot;noun phrase (pronoun), verb&quot; and &quot;verb, noun phrase (pronoun)&quot;. [CLS] [SEP] Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). [CLS] [SEP] These scores have been determined experimentally on an empirical basis and are constantly being up­ dated. [CLS] [SEP] Top symptoms like &quot;lexical reiteration&quot; as­ sign score &quot;2&quot; whereas &quot;non-prepositional&quot; noun phrases are given a negative score of &quot;-1&quot;. [CLS] [SEP] We should point out that the antecedent indicators are preferences and not absolute factors. [CLS] [SEP] For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. [CLS] [SEP] 3.1 Evaluation A. Our first evaluation exercise (Mitkov &amp; Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). [CLS] [SEP] There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. [CLS] [SEP] The resolution of anaphors was carried out with a sucÂ­ cess rate of 95.8%. [CLS] [SEP] The approach being robust (an attempt is made to resolve each anaphor and a pro­ posed antecedent is returned), this figure represents both &quot;precision&quot; and &quot;recall&quot; if we use the MUC terminology. [CLS] [SEP] In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor. [CLS] [SEP] The success rate of the &quot;Baseline Subject&quot; was 29.2%, whereas the success rate of &quot;Baseline Most Recent NP&quot; was 62.5%. [CLS] [SEP] The evaluation carried out was manual to ensure that no added error was genÂ­ erated (e.g. due to possible wrong sentence/clause detection or POS tagging). [CLS] [SEP] Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin&apos;s method, which not being available to us, had to be hand-simulated (see 3.3). [CLS] [SEP] The evaluation indicated 83.6% success rate. [CLS] [SEP] The evaluation indicated 83.6% success rate. [CLS] [SEP] It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary. [CLS] [SEP] We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. [CLS] [SEP] Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. [CLS] [SEP] We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).