[CLS] [SEP] This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby. [CLS] [SEP] The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text [CLS] [SEP] The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. [CLS] [SEP] Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen [CLS] [SEP] A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson&apos;sX 2 statistic. [CLS] [SEP] However, (Cressie and Read, 1984) suggest that there are cases where Pearson&apos;s statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other. [CLS] [SEP] Unfortunately it is usually not clear which test is most appropriate for a particular sample of data. [CLS] [SEP] We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests. [CLS] [SEP] Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems. [CLS] [SEP] Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice CoeÆcient. [CLS] [SEP] While the accuracy of this approach was as good as any previously published results, the learned models were complex and diÆcult to interpret, in e?ect acting as very accurate black boxes. [CLS] [SEP] This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.