[CLS] [SEP] Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. [CLS] [SEP] Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). [CLS] [SEP] Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. [CLS] [SEP] Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. [CLS] [SEP] For example, the word shot in â€œIt was a nice shot.â€ should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. [CLS] [SEP] In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs. [CLS] [SEP] Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. [CLS] [SEP] We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. [CLS] [SEP] The translation lexicon p(f |e) is the key component in this generative process. [CLS] [SEP] We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework. [CLS] [SEP] Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity [CLS] [SEP] Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ). [CLS] [SEP] The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). [CLS] [SEP] To reduce the data sparsity problem, we introduce two remedies in our models. [CLS] [SEP] First: Laplace smoothing. [CLS] [SEP] Second: interpolation smoothing. [CLS] [SEP] Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: [CLS] [SEP] Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA). [CLS] [SEP] Inter takes the intersection of the two directions and generates high-precision alignments; [CLS] [SEP] Topic-specific translation lexicons are learned by a 3-topic BiTAM1. [CLS] [SEP] Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE). [CLS] [SEP] As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1∼3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1.