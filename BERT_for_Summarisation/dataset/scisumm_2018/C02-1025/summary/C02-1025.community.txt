It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.
Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.
In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.
A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.
We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.
We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).
Such constraints are derived from training data, expressing some relationship between features and outcome.
The features we used can be divided into 2 classes: local and global.
Local features are features that are based on neighboring tokens, as well as the token itself.
Local features are features that are based on neighboring tokens, as well as the token itself.
Global features are extracted from other occurrences of the same token in the whole document. 
In the maximum entropy framework, there is no such constraint.
Multiple features can be used for the same token.
Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.
For all lists except locations, the lists are processed into a list of tokens (unigrams).
Location list is processed into a list of unigrams and bigrams (e.g., New York).
For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.
The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.
For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.
We have shown that the maximum entropy framework is able to use global information directly.