Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.
BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.
These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.
The focus of our work is on the use of contextual role knowledge for coreference resolution.
A contextual role represents the role that a noun phrase plays in an event or relationship.
We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.
BABAR employs information extraction techniques to represent and learn role relationships.
Each pattern represents the role that a noun phrase plays in the surrounding context.
Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another.1 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved.
Ex: Mr. Bush disclosed the policy by reading it
Table 1: Syntactic Seeding Heuristics BABARâ€™s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.
Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.
Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes
We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain.
Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found.
We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus.
2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions.
For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphorâ€™s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions.
If so, the CF Network reports that the anaphor and candidate may be coreferent.
One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves.
The confidence level is then used as the belief value for the knowledge source.
Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.
Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.
We evaluated BABAR on two domains: terrorism and natural disasters.
The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision.
The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters.