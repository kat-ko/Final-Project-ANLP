{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from rouge_metric import PyRouge\n",
    "import xmltodict\n",
    "import untangle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from num2words import num2words \n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"C:/Users/Lenovo/Downloads/scisumm/\")\n",
    "BASE_DIR = \"C:/Users/Lenovo/Downloads/scisumm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file C00-2123.xml\n",
      "Summary C00-2123.community.txt\n",
      "file C02-1025.xml\n",
      "Summary C02-1025.community.txt\n",
      "file C04-1089.xml\n",
      "Summary C04-1089.community.txt\n",
      "file C08-1098.xml\n",
      "Summary C08-1098.community.txt\n",
      "file C10-1045.xml\n",
      "Summary C10-1045.community.txt\n",
      "file C90-2039.xml\n",
      "Summary C90-2039.community.txt\n",
      "file C94-2154.xml\n",
      "Summary C94-2154.community.txt\n",
      "file D10-1083.xml\n",
      "Summary D10-1083.community.txt\n",
      "file E03-1020.xml\n",
      "Summary E03-1020.community.txt\n",
      "file E09-2008.xml\n",
      "Summary E09-2008.community.txt\n",
      "file H05-1115.xml\n",
      "Summary H05-1115.community.txt\n",
      "file H89-2014.xml\n",
      "Summary H89-2014.community.txt\n",
      "file I05-5011.xml\n",
      "Summary I05-5011.community.txt\n",
      "file J00-3003.xml\n",
      "Summary J00-3003.community.txt\n",
      "file J96-3004.xml\n",
      "Summary J96-3004.community.txt\n",
      "file J98-2005.xml\n",
      "Summary J98-2005.community.txt\n",
      "file N01-1011.xml\n",
      "Summary N01-1011.summary.txt\n",
      "file N04-1038.xml\n",
      "Summary N04-1038.community.txt\n",
      "file N06-2049.xml\n",
      "Summary N06-2049-community.txt\n",
      "file P05-1004.xml\n",
      "Summary P05-1004.community.txt\n",
      "file P05-1053.xml\n",
      "Summary P05-1053.community.txt\n",
      "file P06-2124.xml\n",
      "Summary P06-2124.community.txt\n",
      "file P98-1046.xml\n",
      "Summary P98-1046.community.txt\n",
      "file P98-1081.xml\n",
      "Summary P98-1081.community.txt\n",
      "file P98-2143.xml\n",
      "Summary P98-2143.community.txt\n",
      "file W03-0410.xml\n",
      "Summary W03-0410.community.txt\n",
      "file W04-0213.xml\n",
      "Summary W04-0213.community.txt\n",
      "file W08-2222.xml\n",
      "Summary W08-2222.community.txt\n",
      "file W95-0104.xml\n",
      "Summary W95-010.community.txt\n",
      "file X96-1048.xml\n",
      "Summary X96-1048.community.txt\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir(BASE_DIR):\n",
    "    for subfolder in os.listdir(BASE_DIR + folder):\n",
    "        for filename in os.listdir(BASE_DIR + folder + \"/\" + subfolder):\n",
    "            if(subfolder == \"Reference_XML\"):\n",
    "                print(\"file\", filename)\n",
    "                xml_dict = xmltodict.parse(open(os.path.join(BASE_DIR + folder + \"/\" + subfolder + \"/\" + filename),encoding='utf-8', errors='ignore').read())\n",
    "                articles.append(xml_dict)\n",
    "            else:\n",
    "                #print(os.path.join(BASE_DIR + folder + \"/\" + subfolder + \"/\" + filename))\n",
    "                print(\"Summary\", filename)\n",
    "                summary = open(os.path.join(BASE_DIR + folder + \"/\" + subfolder + \"/\" + filename), encoding='utf-8', errors='ignore').read()\n",
    "                summaries.append(summary)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the summary into sentences and removig empty string\n",
    "summaries_split = []\n",
    "\n",
    "for i in range(len(summaries)):\n",
    "    summaries_split.insert(i, summaries[i].split(\"\\n\"))\n",
    "    while True:\n",
    "        try:\n",
    "            summaries_split[i].remove(\"\")\n",
    "        except ValueError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in Text: i 5 j 6\n",
      "Exception in Text: i 10 j 7\n",
      "Exception in Text: i 17 j 2\n",
      "Exception in Text: i 18 j 5\n",
      "Exception in Text: i 23 j 3\n",
      "Exception in Text: i 28 j 5\n",
      "Exception in Abstract: i 29\n"
     ]
    }
   ],
   "source": [
    "papers = []\n",
    "\n",
    "'''\n",
    "Produce Data Structure from xlm Data\n",
    "\n",
    "Exceptions occur, if one of the xml-tags is empty\n",
    "'''\n",
    "for i in range(len(articles)):\n",
    "    paper = []\n",
    "    for key in articles[0]['PAPER'].keys():\n",
    "        if(key == 'S'):\n",
    "            title = articles[0]['PAPER']['S']['#text']\n",
    "            #append title to paper\n",
    "            paper.insert(0, title)\n",
    "        \n",
    "        text_sentences = []\n",
    "\n",
    "        # Shuffle through all sentences in the abstract\n",
    "        if(key == 'ABSTRACT'):\n",
    "            try:\n",
    "                for j in range(len(articles[i]['PAPER']['ABSTRACT']['S'])):\n",
    "                        sentence = articles[i]['PAPER']['ABSTRACT']['S'][j]['#text']\n",
    "                        text_sentences.append(sentence)\n",
    "            except:\n",
    "                print(\"Exception in Abstract:\", \"i\", i)\n",
    "\n",
    "\n",
    "        # Shuffle through all sentences in the sections - introduction, methods, results, ...\n",
    "        # articles[0]['PAPER']['SECTION'][0]['S'][0]['#text']\n",
    "        if(key == 'SECTION'):\n",
    "            try:\n",
    "                for j in range(len(articles[i]['PAPER']['SECTION'])):\n",
    "                        for k in range(len(articles[i]['PAPER']['SECTION'][j]['S'])):\n",
    "                            sentence = articles[i]['PAPER']['SECTION'][j]['S'][k]['#text']\n",
    "                            text_sentences.append(sentence)\n",
    "            except:\n",
    "                print(\"Exception in Text:\", \"i\", i, \"j\", j)\n",
    "            \n",
    "        #append sentences to paper\n",
    "        paper.insert(1, text_sentences)\n",
    "        paper.insert(2, summaries_split[i])\n",
    "\n",
    "    papers.append(paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW TO HANDLE THE DATA NOW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word Re-ordering and DP-based Search in Statistical Machine Translation'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "papers = list of papers\n",
    "    Sub-Indices:\n",
    "    [0] - title\n",
    "    [1] - abstract\n",
    "    [2] - text\n",
    "'''\n",
    "\n",
    "# Access the title of the first paper\n",
    "papers[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The goal of machine translation is the translation of a text given in some source language into a target language.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the text of the first paper\n",
    "papers[0][1]\n",
    "\n",
    "# Access the first sentence of the text of the first paper\n",
    "papers[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The goal of machine translation is the translation of a text given in some source language into a target language.', 'We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.', 'Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model.', 'Our approach uses word-to-word dependencies between source and target words.', 'The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).', 'These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition.', 'The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.', 'A simple extension will be used to handle this problem.', 'In Section 2, we brie y review our approach to statistical machine translation.', 'In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.', 'This approach is compared to another reordering scheme presented in (Berger et al., 1996).', 'In Section 4, we present the performance measures used and give translation results on the Verbmobil task.', 'In this section, we brie y review our translation approach.', 'In Eq.', '(1), Pr(eI 1) is the language model, which is a trigram language model in this case.', 'For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.', 'The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\\U001000001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\\U001000001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).', 'When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect.', 'In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.', '2.1 Inverted Alignments.', 'To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).', 'An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi.', \"What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\\U001000001 i\\U001000002) max bI 1 I Yi=1 [p(bijbi\\U001000001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\\U001000001 i\\U001000002) p(bijbi\\U001000001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\\U001000001 i\\U001000002) is the trigram language model probability.\", 'The inverted alignment probability p(bijbi\\U001000001; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration.', 'The details are given in (Och and Ney, 2000).', 'The sentence length probability p(JjI) is omitted without any loss in performance.', 'For the inverted alignment probability p(bijbi\\U001000001; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining.', \"The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment.\", 'We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.', 'The word joining is done on the basis of a likelihood criterion.', 'An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.', \"E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment.\", 'In the following, we assume that this word joining has been carried out.', 'Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup.', 'In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).', 'The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1.', 'A straightforward way to find the shortest tour is by trying all possible permutations of the n cities.', 'The resulting algorithm has a complexity of O(n!).', 'However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.', 'The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city.', 'Subsets C of increasing cardinality c are processed.', 'The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.', 'For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.', 'This algorithm can be applied to statistical machine translation.', 'Using the concept of inverted alignments, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed.', 'The advantage is that we can recombine search hypotheses by dynamic programming.', 'The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.', 'input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max Æ;e00 j02Cnfjg fp(jjj0; J) p(Æ) pÆ(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once.', 'Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed.', 'For a trigram language model, the partial hypotheses are of the form (e0; e; C; j).', 'e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.', 'Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities.', 'The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and ei\\U001000001 = e0.', 'The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either Æ = 0 or Æ = 1 new target words.', 'For Æ = 1, a new target language word is generated using the trigram language model p(eje0; e00).', 'For Æ = 0, no new target word is generated, while an additional source sentence position is covered.', 'A modified language model probability pÆ(eje0; e00) is defined as follows: pÆ(eje0; e00) =  1:0 if Æ = 0 p(eje0; e00) if Æ = 1 : We associate a distribution p(Æ) with the two cases Æ = 0 and Æ = 1 and set p(Æ = 1) = 0:7.', 'The above auxiliary quantity satisfies the following recursive DP equation: Qe0 (e; C; j) = Initial Skip Verb Final 1.', 'In.', '2.', 'diesem 3.', 'Fall.', '4.', 'mein 5.', 'Kollege.', '6.', 'kann 7.nicht 8.', 'besuchen 9.', 'Sie.', '10.', 'am 11.', 'vierten 12.', 'Mai.', '13.', 'Figure 2: Order in which source positions are visited for the example given in Fig.1.', '= p(fj je) max Æ;e00 j02Cnfjg np(jjj0; J) p(Æ) pÆ(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j).', 'The resulting algorithm is depicted in Table 1.', 'The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.', '3.1 Word ReOrdering with Verbgroup.', 'Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.', 'On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.', 'No: Predecessor coverage set Successor coverage set 1 (f1; ;mg n flg ; l0) !', '(f1; ;mg ; l) 2 (f1; ;mg n fl; l1g ; l0) !', '(f1; ;mg n fl1g ; l) 3 (f1; ;mg n fl; l1; l2g ; l0) !', '(f1; ;mg n fl1; l2g ; l) 4 (f1; ;m \\U00100000 1g n fl1; l2; l3g ; l0) !', '(f1; ;mg n fl1; l2; l3g ;m) German to English the monotonicity constraint is violated mainly with respect to the German verbgroup.', 'In German, the verbgroup usually consists of a left and a right verbal brace, whereas in English the words of the verbgroup usually form a sequence of consecutive words.', 'Our new approach, which is called quasi-monotone search, processes the source sentence monotonically, while explicitly taking into account the positions of the German verbgroup.', 'A typical situation is shown in Figure 1.', \"When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.\", \"Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated.\", 'The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions.', 'To formalize the approach, we introduce four verbgroup states S: Initial (I): A contiguous, initial block of source positions is covered.', 'Skipped (K): The translation of up to one word may be postponed . Verb (V): The translation of up to two words may be anticipated.', 'Final (F): The rest of the sentence is processed monotonically taking account of the already covered positions.', 'While processing the source sentence monotonically, the initial state I is entered whenever there are no uncovered positions to the left of the rightmost covered position.', 'The sequence of states needed to carry out the word reordering example in Fig.', '1 is given in Fig.', '2.', 'The 13 positions of the source sentence are processed in the order shown.', 'A position is presented by the word at that position.', 'Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) !', '(S; C; j); Not only the coverage set C and the positions j; j0, but also the verbgroup states S; S0 are taken into account.', 'To be short, we omit the target words e; e0 in the formulation of the search hypotheses.', 'There are 13 types of extensions needed to describe the verbgroup reordering.', 'The details are given in (Tillmann, 2000).', 'For each extension a new position is added to the coverage set.', 'Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).', 'Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence.', 'The search starts in the hypothesis (I; f;g; 0).', 'f;g denotes the empty set, where no source sentence position is covered.', 'The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Æ;e00 np(jjj0; J) p(Æ) pÆ(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).', 'f1; ; Jg denotes a coverage set including all positions from the starting position 1 to position J and j 2 fJ \\U00100000L; ; Jg.', 'The final score is obtained from: max e;e0 j2fJ\\U00100000L;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.', 'The complexity of the quasimonotone search is O(E3 J (R2+LR)).', 'The proof is given in (Tillmann, 2000).', '3.2 Reordering with IBM Style.', 'Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).', 'A detailed description of the search procedure used is given in this patent.', 'Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1.', 'A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.', 'Here, we process only full-form words within the translation procedure.', 'the number of permutations carried out for the word reordering is given.', 'During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet.', 'Only one of the first n positions which are not already aligned in a partial hypothesis may be chosen, where n is set to 4.', 'The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.', 'This number must be less than or equal to n \\U00100000 1.', 'Otherwise for the predecessor search hypothesis, we would have chosen a position that would not have been among the first n uncovered positions.', 'Ignoring the identity of the target language words e and e0, the possible partial hypothesis extensions due to the IBM restrictions are shown in Table 2.', 'In general, m; l; l0 6= fl1; l2; l3g and in line umber 3 and 4, l0 must be chosen not to violate the above reordering restriction.', 'Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction.', 'A dynamic programming recursion similar to the one in Eq. 2 is evaluated.', 'In this case, we have no finite-state restrictions for the search space.', 'The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg.', 'This approach leads to a search procedure with complexity O(E3 J4).', 'The proof is given in (Tillmann, 2000).', '4.1 The Task and the Corpus.', 'We have tested the translation system on the Verbmobil task (Wahlster 1993).', 'The Verbmobil task is an appointment scheduling task.', 'Two subjects are each given a calendar and they are asked to schedule a meeting.', 'The translation direction is from German to English.', 'A summary of the corpus used in the experiments is given in Table 3.', 'The perplexity for the trigram language model used is 26:5.', 'Although the ultimate goal of the Verbmobil project is the translation of spoken language, the input used for the translation experiments reported on in this paper is the (more or less) correct orthographic transcription of the spoken sentences.', 'Thus, the effects of spontaneous speech are present in the corpus, e.g. the syntactic structure of the sentence is rather less restricted, however the effect of speech recognition errors is not covered.', 'For the experiments, we use a simple preprocessing step.', 'German city names are replaced by category markers.', 'The translation search is carried out with the category markers and the city names are resubstituted into the target sentence as a postprocessing step.', 'Table 3: Training and test conditions for the Verbmobil task (*number of words without punctuation marks).', 'German English Training: Sentences 58 073 Words 519 523 549 921 Words* 418 979 453 632 Vocabulary Size 7939 4648 Singletons 3454 1699 Test-147: Sentences 147 Words 1 968 2 173 Perplexity { 26:5 Table 4: Multi-reference word error rate (mWER) and subjective sentence error rate (SSER) for three different search procedures.', 'Search CPU time mWER SSER Method [sec] [%] [%] MonS 0:9 42:0 30:5 QmS 10:6 34:4 23:8 IbmS 28:6 38:2 26:2 4.2 Performance Measures.', 'The following two error criteria are used in our experiments: mWER: multi-reference WER: We use the Levenshtein distance between the automatic translation and several reference translations as a measure of the translation errors.', 'On average, 6 reference translations per automatic translation are available.', 'The Levenshtein distance between the automatic translation and each of the reference translations is computed, and the minimum Levenshtein distance is taken.', 'This measure has the advantage of being completely automatic.', 'SSER: subjective sentence error rate: For a more detailed analysis, the translations are judged by a human test person.', 'For the error counts, a range from 0:0 to 1:0 is used.', 'An error count of 0:0 is assigned to a perfect translation, and an error count of 1:0 is assigned to a semantically and syntactically wrong translation.', '4.3 Translation Experiments.', 'For the translation experiments, Eq. 2 is recursively evaluated.', 'We apply a beam search concept as in speech recognition.', 'However there is no global pruning.', 'Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.', 'Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited.', 'For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÆcient to consider only the best 50 words.', 'We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.', 'Table 4 shows translation results for the three approaches.', 'The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC).', 'Here, the pruning threshold t0 = 10:0 is used.', 'Translation errors are reported in terms of multireference word error rate (mWER) and subjective sentence error rate (SSER).', 'The monotone search performs worst in terms of both error rates mWER and SSER.', 'The computing time is low, since no reordering is carried out.', 'The quasi-monotone search performs best in terms of both error rates mWER and SSER.', 'Additionally, it works about 3 times as fast as the IBM style search.', 'For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.', 'The effect of the pruning threshold t0 is shown in Table 5.', 'The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.', 'The negative logarithm of t0 is reported.', 'The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series, we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0, and this number is reported as the number of search errors.', 'Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors.', 'Decreasing the threshold results in higher mWER due to additional search errors.', 'Table 5: Effect of the beam threshold on the number of search errors (147 sentences).', 'Search t0 CPU time #search mWER Method [sec] error [%] QmS 0.0 0.07 108 42:6 1.0 0.13 85 37:8 2.5 0.35 44 36:6 5.0 1.92 4 34:6 10.0 10.6 0 34:5 IbmS 0.0 0.14 108 43:4 1.0 0.3 84 39:5 2.5 0.8 45 39:1 5.0 4.99 7 38:3 10.0 28.52 0 38:2 Table 6 shows example translations obtained by the three different approaches.', 'Again, the monotone search performs worst.', 'In the second and third translation examples, the IbmS word reordering performs worse than the QmS word reordering, since it can not take properly into account the word reordering due to the German verbgroup.', \"The German finite verbs 'bin' (second example) and 'k\\x7fonnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions).\", 'In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable.', 'In this paper, we have presented a new, eÆcient DP-based search procedure for statistical machine translation.', 'The approach assumes that the word reordering is restricted to a few positions in the source sentence.', 'The approach has been successfully tested on the 8 000-word Verbmobil task.', 'Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.', '2) An improved language model, which takes into account syntactic structure, e.g. to ensure that a proper English verbgroup is generated.', '3) A tight coupling with the speech recognizer output.', 'This work has been supported as part of the Verbmobil project (contract number 01 IV 601 A) by the German Federal Ministry of Education, Science, Research and Technology and as part of the Eutrans project (ESPRIT project number 30268) by the European Community.', 'Table 6: Example Translations for the Verbmobil task.', 'Input: Ja , wunderbar . K\\x7fonnen wir machen . MonS: Yes, wonderful.', 'Can we do . QmS: Yes, wonderful.', 'We can do that . IbmS: Yes, wonderful.', 'We can do that . Input: Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin . Genaugenommen nur am dritten . Wie w\\x7fare es denn am \\x7fahm Samstag , dem zehnten Februar ? MonS: That is too tight , because I from the third in Kaiserslautern . In fact only on the third . How about \\x7fahm Saturday , the tenth of February ? QmS: That is too tight , because I am from the third in Kaiserslautern . In fact only on the third . \\x7fAhm how about Saturday , February the tenth ? IbmS: That is too tight , from the third because I will be in Kaiserslautern . In fact only on the third . \\x7fAhm how about Saturday , February the tenth ? Input: Wenn Sie dann noch den siebzehnten k\\x7fonnten , w\\x7fare das toll , ja . MonS: If you then also the seventeenth could , would be the great , yes . QmS: If you could then also the seventeenth , that would be great , yes . IbmS: Then if you could even take seventeenth , that would be great , yes . Input: Ja , das kommt mir sehr gelegen . Machen wir es dann am besten so . MonS: Yes , that suits me perfectly . Do we should best like that . QmS: Yes , that suits me fine . We do it like that then best . IbmS: Yes , that suits me fine . We should best do it like that .']\n"
     ]
    }
   ],
   "source": [
    "# Access the summary of the first paper\n",
    "papers[0][2]\n",
    "\n",
    "# Access the first sentence of the summary of the first paper\n",
    "papers[0][2][0]\n",
    "\n",
    "print(papers[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the functions for TF-IDF and SMMRY calculations\n",
    "\n",
    "#Creating unique words list\n",
    "def find_unique_words(sentences): \n",
    "    unique_words = list()\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence) \n",
    "        for word in words:\n",
    "            if word not in unique_words:\n",
    "                unique_words.append(word)    \n",
    "    return unique_words  \n",
    "    \n",
    "#Creating frequency matrix of the words in each sentence\n",
    "def create_freq_matrix(sentences):\n",
    "    freq = dict()\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        freq_dict = dict()\n",
    "\n",
    "        for word in words:\n",
    "            if word in freq_dict:\n",
    "                freq_dict[word] += 1\n",
    "            else:\n",
    "                freq_dict[word] = 1\n",
    "\n",
    "        freq[sentence] = freq_dict   \n",
    "    return freq\n",
    "    \n",
    "#TF\n",
    "def find_tf(freq):\n",
    "    tf = dict()\n",
    "    for sentence in freq:\n",
    "        words = word_tokenize(sentence)\n",
    "        tf_dict = dict()\n",
    "        for word in words:\n",
    "            tf_dict[word] = freq[sentence][word] /len(words)\n",
    "        tf[sentence] = tf_dict\n",
    "    return tf\n",
    "    \n",
    "#IDF\n",
    "def find_idf(uninique_words, sentences):\n",
    "    idf = dict()\n",
    "    for word in unique_words:\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence) \n",
    "            if word in set(words):\n",
    "                if word in idf:\n",
    "                    idf[word] += 1\n",
    "                else:\n",
    "                    idf[word] = 1         \n",
    "    for word in idf:\n",
    "        idf[word] = np.log(len(sentences)/idf[word])\n",
    "    return idf\n",
    "\n",
    "#Combining TF IDF\n",
    "def find_tf_idf(tf, idf):\n",
    "    tf_idf = dict()\n",
    "    for sentence in tf:\n",
    "        words = word_tokenize(sentence)\n",
    "        tf_idf_dict = dict()\n",
    "        for word in words:\n",
    "            tf_idf_dict[word] = tf[sentence][word] * idf[word]\n",
    "        tf_idf[sentence] = tf_idf_dict\n",
    "    return tf_idf\n",
    "\n",
    "#Finding sentences with highest score\n",
    "def find_top_n_sentences(tf_idf, n, lookup):\n",
    "    #Finding n sentences with higesht score\n",
    "    answer_head = list()\n",
    "    answer_value = list()\n",
    "    for key, value in tf_idf.items():\n",
    "        answer_value.append(sum(value.values()))\n",
    "        answer_head.append(key)\n",
    "\n",
    "    answ = sorted(zip(answer_value, answer_head), reverse=True)[:n]\n",
    "\n",
    "    #Getting not edited versions of the sentences\n",
    "    val, sen = zip(*answ)\n",
    "    for i in range(len(val)):\n",
    "        answ[i] = lookup[sen[i]]\n",
    "    return(answ)\n",
    "    \n",
    "    \n",
    "#Creating an array from the dictionary (including all the words)\n",
    "def create_tf_idf_array_from_dict(tf_idf, sentences, unique_words):\n",
    "    i = len(unique_words)\n",
    "    arr = np.zeros((len(sentences), len(unique_words)))\n",
    "\n",
    "    for i, (key, sentence) in enumerate(tf_idf.items()):\n",
    "        for j, (word, value) in enumerate(sentence.items()):\n",
    "            idx = unique_words.index(word)\n",
    "            arr[i][idx] = value\n",
    "    return arr\n",
    "\n",
    "#Creating similarity matrix        \n",
    "def find_similarity_matrix(X):\n",
    "    cos_sim = np.dot(X,X.T)\n",
    "    np.fill_diagonal(cos_sim, 0) \n",
    "      \n",
    "    for index in range(len(cos_sim)):\n",
    "        if cos_sim[index].sum() == 0:\n",
    "            continue\n",
    "        else:\n",
    "            cos_sim[index] /= cos_sim[index].sum()\n",
    "    return cos_sim\n",
    "\n",
    "#Finding the textrank of the sentence (this function is borrowed from https://github.com/foprel/smmry/blob/master/app.py)\n",
    "def find_textrank(A, eps=0.0001, d=0.85):\n",
    "    P = np.ones(len(A)) / len(A)\n",
    "    while True:\n",
    "        new_P = np.ones(len(A)) * (1 - d) / len(A) + d * A.T.dot(P)\n",
    "        delta = abs(new_P - P).sum()\n",
    "        if delta <= eps:\n",
    "            return new_P\n",
    "        P = new_P\n",
    "\n",
    "def find_max_n_values(sentences_original, text_rank, n):\n",
    "    answ = sorted(zip(text_rank, sentences_original), reverse=True)[:n]\n",
    "    val, sentence = zip(*answ)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences, sentences_original):\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        \n",
    "        #Making Lowercase\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        #Removing everything that is not a letter (punctuations, digits, unknown symbols)\n",
    "        sentence = re.sub(r'[^a-zA-Z ]','',sentence) \n",
    "        \n",
    "        words_list = word_tokenize(sentence)\n",
    "        filtered_words = words_list\n",
    "        \n",
    "        #Removing stopwords\n",
    "        filtered_words = [word for word in words_list if word not in set(stopwords.words(\"english\"))] \n",
    "        \n",
    "        #Removing one/two letter words\n",
    "        filtered_words = [word for word in filtered_words if len(word)>2]\n",
    "                \n",
    "        if (len(filtered_words)<=2 or (not filtered_words)):\n",
    "            sentence_to_be_removed = sentences_original[index]\n",
    "            sentences.pop(index)\n",
    "            sentences_original.pop(index)\n",
    "        else:\n",
    "\n",
    "            #Converting numbers to text\n",
    "            #filtered_words = [num2words(word, lang='en') if word.isdigit() else word for word in filtered_words]\n",
    "\n",
    "            #Remove not english words\n",
    "            #filtered_words = [w for w in filtered_words if w in words.words()]\n",
    "\n",
    "            #Remove digits\n",
    "            #filtered_words = [word for word in filtered_words if not word.isdigit()]\n",
    "\n",
    "            #Stemming (would be nice to try lematisation or both)\n",
    "            ps = PorterStemmer()\n",
    "            filtered_words = [ps.stem(word) for word in filtered_words] \n",
    "\n",
    "            sentence = ' '.join(filtered_words)   \n",
    "            sentences[index] = sentence \n",
    "        \n",
    "    return sentences, sentences_original\n",
    "        \n",
    "        \n",
    "def filtering_tags(preprocessed_tokens, pos=['NN', 'ADJ']):\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words_list = word_tokenize(sentence)\n",
    "        \n",
    "        filtered_words = []\n",
    "        for index, s in enumerate(words_list):\n",
    "            s = pos_tag([s])\n",
    "            if s[0][1] in pos:\n",
    "                filtered_words.append(s[0][0])      \n",
    "        \n",
    "        sentence = ' '.join(filtered_words)   \n",
    "        sentences[i] = sentence   \n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "tf_idf_score = list()\n",
    "smmry_score = list()\n",
    "\n",
    "tf_idf_predicted = list()\n",
    "smmry_predicted = list()\n",
    "\n",
    "for idx, article in enumerate(papers): #enumerate(papers[0:10]): #Select all papers\n",
    "    n = len(papers[idx][2]) #number of sentences to select for summarisation\n",
    "    sentences_original = copy.deepcopy(article[1]) \n",
    "    sentences = copy.deepcopy(article[1])\n",
    "    \n",
    "    #Data pre-processing\n",
    "    sentences, sentences_original = preprocessing(sentences, sentences_original) \n",
    "    sentences_smmry = filtering_tags(sentences)\n",
    "    \n",
    "    #TF-IDF part\n",
    "    lookup = dict()\n",
    "    for i in range(len(sentences)):\n",
    "        lookup[sentences[i]] = sentences_original[i]\n",
    "\n",
    "    unique_words = find_unique_words(sentences)\n",
    "    freq_matrix = create_freq_matrix(sentences)\n",
    "    \n",
    "    tf = find_tf(freq_matrix)\n",
    "    idf = find_idf(unique_words, sentences)\n",
    "    tf_idf = find_tf_idf(tf, idf)\n",
    "    top_n_sentences_tf_idf = find_top_n_sentences(tf_idf, n, lookup)\n",
    "    tf_idf_predicted.append(top_n_sentences_tf_idf)\n",
    "\n",
    "    score=0\n",
    "    for i, sent in enumerate(top_n_sentences_tf_idf):\n",
    "        if sent in papers[idx][2]:\n",
    "            score+=1/n\n",
    "    tf_idf_score.append(score)\n",
    "\n",
    "    #SMMRY part\n",
    "    sentences = sentences_smmry\n",
    "    lookup = dict()\n",
    "    for i in range(len(sentences)):\n",
    "        lookup[sentences[i]] = sentences_original[i]\n",
    "\n",
    "    unique_words = find_unique_words(sentences)\n",
    "    freq_matrix = create_freq_matrix(sentences)\n",
    "    \n",
    "    tf = find_tf(freq_matrix)\n",
    "    idf = find_idf(unique_words, sentences)\n",
    "    tf_idf = find_tf_idf(tf, idf)\n",
    "    \n",
    "    tf_idf_array = create_tf_idf_array_from_dict(tf_idf, sentences, unique_words)    \n",
    "    similarity_matrix = find_similarity_matrix(tf_idf_array)\n",
    "    textrank = find_textrank(similarity_matrix)\n",
    "\n",
    "    top_n_sentences_textrank = find_max_n_values(sentences_original, textrank, n)\n",
    "    smmry_predicted.append(top_n_sentences_textrank)\n",
    "\n",
    "    score=0\n",
    "    for i, sent in enumerate(top_n_sentences_textrank):\n",
    "        if sent in papers[idx][2]:\n",
    "            score+=1/n\n",
    "    smmry_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 0.02624724178521452\n",
      "SMMRY 0.07782546052279816 \n",
      "\n",
      "ROUGE metrics:\n",
      "TF-IDF rouge 1 score:  r: 0.0333653291288336  p: 0.0333653291288336 f: 0.002131200632425981\n",
      "TF-IDF rouge 2 score:  r: 0.0  p: 0.0 f: 0.0\n",
      "TF-IDF rouge L score:  r: 0.0333653291288336  p: 0.0333653291288336 f: 0.002131200632425981\n",
      "SMMRY rouge 1 score:  r: 0.02206080145664252  p: 0.02206080145664252 f: 0.0018426351354706373\n",
      "SMMRY rouge 2 score:  r: 0.0  p: 0.0 f: 0.0\n",
      "SMMRY rouge L score:  r: 0.02206080145664252  p: 0.02206080145664252 f: 0.0018426351354706373\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "\n",
    "#Accuracy\n",
    "print(\"TF-IDF\", np.mean(tf_idf_score))\n",
    "print(\"SMMRY\", np.mean(smmry_score),\"\\n\")\n",
    "\n",
    "#ROUGE-1, ROUGE-2 and ROUGE-L metrics\n",
    "rouge = PyRouge(rouge_n=(1, 2), rouge_l=True)\n",
    "print(\"ROUGE metrics:\")\n",
    "\n",
    "tf_idf_rouge_1_r = list(); tf_idf_rouge_1_p = list(); tf_idf_rouge_1_f = list()\n",
    "tf_idf_rouge_2_r = list(); tf_idf_rouge_2_p = list(); tf_idf_rouge_2_f = list()\n",
    "tf_idf_rouge_l_r = list(); tf_idf_rouge_l_p = list(); tf_idf_rouge_l_f = list()\n",
    "\n",
    "smmry_rouge_1_r = list(); smmry_rouge_1_p = list(); smmry_rouge_1_f = list()\n",
    "smmry_rouge_2_r = list(); smmry_rouge_2_p = list(); smmry_rouge_2_f = list()\n",
    "smmry_rouge_l_r = list(); smmry_rouge_l_p = list(); smmry_rouge_l_f = list()\n",
    "\n",
    "for i in range(len(tf_idf_predicted)):\n",
    "    \n",
    "    scores_tf_idf = rouge.evaluate(tf_idf_predicted[i], papers[i][2])\n",
    "    tf_idf_rouge_1_r.append(scores_tf_idf['rouge-1']['r'])\n",
    "    tf_idf_rouge_2_r.append(scores_tf_idf['rouge-2']['r'])\n",
    "    tf_idf_rouge_l_r.append(scores_tf_idf['rouge-l']['r'])\n",
    "    \n",
    "    tf_idf_rouge_1_p.append(scores_tf_idf['rouge-1']['p'])\n",
    "    tf_idf_rouge_2_p.append(scores_tf_idf['rouge-2']['p'])\n",
    "    tf_idf_rouge_l_p.append(scores_tf_idf['rouge-l']['p'])\n",
    "    \n",
    "    tf_idf_rouge_1_f.append(scores_tf_idf['rouge-1']['f'])\n",
    "    tf_idf_rouge_2_f.append(scores_tf_idf['rouge-2']['f'])\n",
    "    tf_idf_rouge_l_f.append(scores_tf_idf['rouge-l']['f'])\n",
    "    \n",
    "    \n",
    "    scores_smmry = rouge.evaluate(smmry_predicted[i], papers[i][2])\n",
    "    smmry_rouge_1_r.append(scores_smmry['rouge-1']['r'])\n",
    "    smmry_rouge_2_r.append(scores_smmry['rouge-2']['r'])\n",
    "    smmry_rouge_l_r.append(scores_smmry['rouge-l']['r'])\n",
    "    \n",
    "    smmry_rouge_1_p.append(scores_smmry['rouge-1']['p'])\n",
    "    smmry_rouge_2_p.append(scores_smmry['rouge-2']['p'])\n",
    "    smmry_rouge_l_p.append(scores_smmry['rouge-l']['p'])\n",
    "    \n",
    "    smmry_rouge_1_f.append(scores_smmry['rouge-1']['f'])\n",
    "    smmry_rouge_2_f.append(scores_smmry['rouge-2']['f'])\n",
    "    smmry_rouge_l_f.append(scores_smmry['rouge-l']['f'])\n",
    "      \n",
    "print(\"TF-IDF rouge 1 score: \", \"r:\", np.mean(tf_idf_rouge_1_r),\" p:\", np.mean(tf_idf_rouge_1_r), \"f:\", np.mean(tf_idf_rouge_1_f)) \n",
    "print(\"TF-IDF rouge 2 score: \", \"r:\", np.mean(tf_idf_rouge_2_r),\" p:\", np.mean(tf_idf_rouge_2_r), \"f:\", np.mean(tf_idf_rouge_2_f))\n",
    "print(\"TF-IDF rouge L score: \", \"r:\", np.mean(tf_idf_rouge_l_r),\" p:\", np.mean(tf_idf_rouge_l_r), \"f:\", np.mean(tf_idf_rouge_l_f))\n",
    "print(\"SMMRY rouge 1 score: \", \"r:\", np.mean(smmry_rouge_1_r),\" p:\", np.mean(smmry_rouge_1_r), \"f:\", np.mean(smmry_rouge_1_f)) \n",
    "print(\"SMMRY rouge 2 score: \", \"r:\", np.mean(smmry_rouge_2_r),\" p:\", np.mean(smmry_rouge_2_r), \"f:\", np.mean(smmry_rouge_2_f))\n",
    "print(\"SMMRY rouge L score: \", \"r:\", np.mean(smmry_rouge_l_r),\" p:\", np.mean(smmry_rouge_l_r), \"f:\", np.mean(smmry_rouge_l_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of TF-IDF from sklearn: 0.0459185287715282\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of TF-IDF from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "score_list = list()\n",
    "for p in papers:\n",
    "\n",
    "    #Creating TF-IDF vector\n",
    "    tf_idf_vect = TfidfVectorizer()\n",
    "    X_train_tf_idf = tf_idf_vect.fit_transform(p[1])\n",
    "    terms = tf_idf_vect.get_feature_names()\n",
    "    \n",
    "    #Converting it to pandas dataframe\n",
    "    df = pd.DataFrame(X_train_tf_idf.toarray(), columns=terms, index=p[1])\n",
    "    \n",
    "    #Summing values of each sentence\n",
    "    count = pd.DataFrame(df[df!=0].count(axis=1))\n",
    "    df = pd.DataFrame(df.sum(axis=1))\n",
    "    df.columns = ['score']\n",
    "    df[\"count\"] = count\n",
    "    df = pd.DataFrame(df.sort_values(by=\"score\", ascending=False))\n",
    "    \n",
    "    #Calculating accuracy\n",
    "    score = 0\n",
    "    for item in df.iloc[0:10].index:\n",
    "        if item in p[2]:\n",
    "            score+=1\n",
    "    score/=len(p[2])\n",
    "    score_list.append(score)\n",
    "\n",
    "print(\"Accuracy of TF-IDF from sklearn:\", np.mean(score_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
